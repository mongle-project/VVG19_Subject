# ğŸ¯ VGG19 Transfer Learning & Model Compression Project

VGG19 ëª¨ë¸ì˜ **ì „ì´í•™ìŠµ(Transfer Learning)** íš¨ê³¼ë¥¼ ê²€ì¦í•˜ê³ , **ëª¨ë¸ ê²½ëŸ‰í™”(Model Compression)** ë¥¼ í†µí•´ ì ì€ ë°ì´í„°ë¡œ íš¨ìœ¨ì ì¸ ì´ë¯¸ì§€ ë¶„ë¥˜ ëª¨ë¸ì„ êµ¬ì¶•í•œ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.

---

## ğŸ“Œ í”„ë¡œì íŠ¸ ê°œìš”

### ëª©í‘œ
- VGG19 ë…¼ë¬¸ ê¸°ë°˜ ëª¨ë¸ êµ¬í˜„ ë° ì „ì´í•™ìŠµ íš¨ê³¼ ë¹„êµ
- ì ì€ ë°ì´í„°ì…‹(~900ì¥)ì—ì„œ ë†’ì€ ì„±ëŠ¥ ë‹¬ì„±
- ëª¨ë¸ ê²½ëŸ‰í™”ë¥¼ í†µí•œ íš¨ìœ¨ì„± ê°œì„ 

### ë°ì´í„°ì…‹
- **í´ë˜ìŠ¤**: Santa / Normal (2-class classification)
- **ë°ì´í„° êµ¬ì„±**: Train (~900ì¥) / Validation / Test
- **ì „ì²˜ë¦¬**: 224Ã—224 ë¦¬ì‚¬ì´ì¦ˆ, ì •ê·œí™”, ë°ì´í„° ì¦ê°•

---

## ğŸ”¬ ì‹¤í—˜ 1: VGG19 ì „ì´í•™ìŠµ íš¨ê³¼ ê²€ì¦

### 1ï¸âƒ£ ë…¼ë¬¸ ê¸°ë°˜ VGG19 êµ¬í˜„ (From Scratch)

**ëª¨ë¸ êµ¬ì¡°**:
```python
class VGG19(nn.Module):
    def __init__(self, num_classes=2):
        super(VGG19, self).__init__()
        self.features = nn.Sequential(
            # 5ê°œ Conv Block (16ê°œ Conv Layer)
            # 64 â†’ 128 â†’ 256 â†’ 512 â†’ 512
            # ê° ë¸”ë¡ë§ˆë‹¤ MaxPooling
            ...
        )
        
        self.classifier = nn.Sequential(
            nn.Linear(512 * 7 * 7, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(4096, num_classes)
        )
```

**ê²°ê³¼**:
- âŒ **í•™ìŠµ ì‹¤íŒ¨**: Loss = NaN, Accuracy â‰ˆ 50% (ëœë¤ ìˆ˜ì¤€)
- **ì›ì¸**: ë°ì´í„° ë¶€ì¡± (~900ì¥) + íŒŒë¼ë¯¸í„° ê³¼ë‹¤ (1ì–µ+)

---

### 2ï¸âƒ£ ì „ì´í•™ìŠµ ì ìš© (Transfer Learning)

**ì „ëµ**:
```python
# ImageNet ì‚¬ì „ í•™ìŠµ ê°€ì¤‘ì¹˜ ë¡œë“œ
weights = VGG19_Weights.IMAGENET1K_V1
model = vgg19(weights=weights)

# Features (Conv layers) ê³ ì •
for param in model.features.parameters():
    param.requires_grad = False

# Classifierë§Œ ì¬ì •ì˜ (2-class)
model.classifier = nn.Sequential(
    nn.Linear(512 * 7 * 7, 4096),
    nn.ReLU(True),
    nn.Dropout(0.5),
    nn.Linear(4096, 4096),
    nn.ReLU(True),
    nn.Dropout(0.5),
    nn.Linear(4096, 2)
)
```

**ê²°ê³¼**:
- âœ… **Epoch 1**: 91.76% â†’ **ìµœì¢…**: ~97% ì •í™•ë„
- Optimizer ë¹„êµ:
  - **Adam**: ì•ˆì •ì  í•™ìŠµ, 97% ë‹¬ì„±
  - **SGD**: Epoch 12 ì´í›„ ë°œì‚° (Gradient Exploding)

---

## ğŸš€ ì‹¤í—˜ 2: ëª¨ë¸ ê²½ëŸ‰í™” (Model Compression)

### VGG19 ê²½ëŸ‰í™” ë²„ì „ êµ¬í˜„

**ì„¤ê³„ ì „ëµ**:
- ì±„ë„ ìˆ˜ ê°ì†Œ: 64â†’128â†’256â†’512 â†’ **32â†’64â†’128â†’256**
- Depth ê°ì†Œ: 5ê°œ ë¸”ë¡ â†’ **4ê°œ ë¸”ë¡**
- BatchNorm ì¶”ê°€ë¡œ í•™ìŠµ ì•ˆì •ì„± í™•ë³´

**BatchNormalization ì ìš© ì´ìœ **:
- ê° ì¸µì˜ ì…ë ¥ ë¶„í¬ë¥¼ ì •ê·œí™”í•˜ì—¬ í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ
- ë” ë†’ì€ Learning Rate ì‚¬ìš© ê°€ëŠ¥
- Gradient Vanishing/Exploding ë°©ì§€ íš¨ê³¼
- ê²½ëŸ‰í™” ëª¨ë¸ì˜ ì„±ëŠ¥ ì €í•˜ ë³´ì™„

**ëª¨ë¸ êµ¬ì¡°**:
```python
class VGG19_Small(nn.Module):
    def __init__(self, num_classes=2):
        super().__init__()
        
        self.features = nn.Sequential(
            # Block 1: 32 channels
            nn.Conv2d(3, 32, 3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(2),
            
            # Block 2: 64 channels
            nn.Conv2d(32, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2),
            
            # Block 3: 128 channels
            nn.Conv2d(64, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.MaxPool2d(2),
            
            # Block 4: 256 channels
            nn.Conv2d(128, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.MaxPool2d(2)
        )
        
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(256 * 14 * 14, 512),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(512, num_classes)
        )
```

**ê²½ëŸ‰í™” íš¨ê³¼**:
- íŒŒë¼ë¯¸í„° ìˆ˜: ~95% ê°ì†Œ
- í•™ìŠµ ì†ë„: ëŒ€í­ ê°œì„ 
- ì„±ëŠ¥: ìœ ì‚¬í•˜ê±°ë‚˜ í–¥ìƒëœ ì •í™•ë„ ìœ ì§€

---

## ğŸ“Š ì‹¤í—˜ ê²°ê³¼ ë¹„êµ

| ëª¨ë¸ | Accuracy | í•™ìŠµ ì•ˆì •ì„± | íŒŒë¼ë¯¸í„° ìˆ˜ |
|------|----------|-------------|-------------|
| VGG19 (From Scratch) | ~50% (ì‹¤íŒ¨) | âŒ NaN | ~138M |
| VGG19 (Transfer Learning) | ~97% | âœ… ì•ˆì • | ~138M |
| VGG19_Small (ê²½ëŸ‰í™”) | ~95-97% | âœ… ì•ˆì • | ~7M |

**í•µì‹¬ ì¸ì‚¬ì´íŠ¸**:
1. **ì „ì´í•™ìŠµì˜ ì¤‘ìš”ì„±**: ì ì€ ë°ì´í„°ì—ì„œ í•„ìˆ˜ì 
2. **Optimizer ì„ íƒ**: Adamì´ SGDë³´ë‹¤ ì•ˆì •ì 
3. **ê²½ëŸ‰í™” íš¨ê³¼**: ì„±ëŠ¥ ìœ ì§€í•˜ë©° íš¨ìœ¨ì„± 95% ê°œì„ 

---

## ğŸ› ï¸ ê¸°ìˆ  ìŠ¤íƒ

- **Framework**: PyTorch
- **Libraries**: torchvision, OpenCV, matplotlib, tqdm
- **Environment**: Google Colab (GPU)

---


## ğŸ“ ì£¼ìš” í•™ìŠµ ë‚´ìš©

### 1. ì „ì´í•™ìŠµ (Transfer Learning)
- ImageNet ì‚¬ì „ í•™ìŠµ ê°€ì¤‘ì¹˜ í™œìš©
- Feature Extraction vs Fine-tuning ì „ëµ
- ì ì€ ë°ì´í„°ì…‹ì—ì„œì˜ íš¨ê³¼ì ì¸ í•™ìŠµ

### 2. ëª¨ë¸ ìµœì í™”
- Optimizer ë¹„êµ (SGD vs Adam)
- Learning rate, momentum í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •
- Gradient Exploding ë¬¸ì œ í•´ê²°

### 3. ëª¨ë¸ ê²½ëŸ‰í™”
- ì±„ë„ ìˆ˜/Depth ì¡°ì •ì„ í†µí•œ íŒŒë¼ë¯¸í„° ê°ì†Œ
- BatchNormalizationì„ í†µí•œ í•™ìŠµ ì•ˆì •ì„± í™•ë³´
- ì„±ëŠ¥ê³¼ íš¨ìœ¨ì„±ì˜ íŠ¸ë ˆì´ë“œì˜¤í”„ ë¶„ì„

---

## ğŸ“ˆ í–¥í›„ ê°œì„  ë°©í–¥

- [ ] Learning Rate Scheduler ì ìš©
- [ ] Early Stopping êµ¬í˜„
- [ ] ë‹¤ë¥¸ ê²½ëŸ‰í™” ê¸°ë²• ì‹¤í—˜ (Knowledge Distillation, Pruning)

---


## ğŸ‘¤ Author

**íŒ€-ë¼ì´ì–¸ì¼ë³‘êµ¬í•˜ê¸°**
- ë”¥ëŸ¬ë‹ ëª¨ë¸ êµ¬í˜„ ë° ìµœì í™”
- ì „ì´í•™ìŠµ íš¨ê³¼ ê²€ì¦ ì‹¤í—˜
